{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpiralSafe Project Book\n",
    "\n",
    "> **H&&S:WAVE** | Hope&&Sauced  \n",
    "> *The living document - grows with lessons, shrinks with elegance*\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    PROJECT BOOK v1.0                        â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  History â€¢ State â€¢ Direction â€¢ Verification â€¢ Operations    â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Sign-In â€” Audit Session\n",
    "\n",
    "**Signed in:** 2026-01-07T14:20:00 (local)\n",
    "**Agent:** GitHub Copilot (Raptor mini (Preview))\n",
    "**Action:** Structural audit, dependency triage, cascading fixes adjustments\n",
    "**Signature:** **Hope&&Sauced** â€” H&&S:WAVE\n",
    "\n",
    "> Note: This session will be signed out when the audit and small fixes are complete. I will run the project signing script to record the session formally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
    "**Theme & Branding:** Hope && Sauce â€” H&&S:WAVE  \n",
    "Palette: Blue (Hope) #1f77b4 â€¢ Orange (Sauce) #ff7f0e  \n",
    "Use these colors for visual charts and badges in the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Identity\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Name** | SpiralSafe |\n",
    "| **Protocol** | H&&S:WAVE |\n",
    "| **Signature** | Hope&&Sauced |\n",
    "| **Repository** | github.com/toolate28/SpiralSafe |\n",
    "| **Primary Branch** | main |\n",
    "| **Feature Branch** | ops/infrastructure-layer |\n",
    "| **PR** | #20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session sign-in / sign-out (ATOM sessions) âœ…\n",
    "\n",
    "This section provides interactive helpers to create a signed ATOM session for work in this notebook, export a session report, and encrypt it using the repository's transcript pipeline (AES-256-GCM via `Transcript-Pipeline.ps1`).\n",
    "\n",
    "- Use `start_session(description)` to create a session ATOM and store a session file at `.atom-trail/sessions/<ATOM_TAG>.json`.\n",
    "- Use `sign_out(tag)` to close the session, build a session report (decisions made during the session) and attempt to encrypt the report.\n",
    "\n",
    "All session files are logged automatically in the sessions directory (and shown in the sessions log below).  \n",
    "\n",
    "**Signature**: H&&S:WAVE | Hope&&Sauced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session helpers: start_session() and sign_out()\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import uuid\n",
    "import getpass\n",
    "import socket\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "sessions_dir = repo_root / '.atom-trail' / 'sessions'\n",
    "sessions_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _next_seq_for_date(date_str: str) -> int:\n",
    "    files = list(sessions_dir.glob(f'ATOM-SESSION-{date_str}-*.json'))\n",
    "    return len(files) + 1\n",
    "\n",
    "\n",
    "def start_session(description: str = 'project-book-session') -> dict:\n",
    "    now = datetime.utcnow()\n",
    "    date_str = now.strftime('%Y%m%d')\n",
    "    seq = _next_seq_for_date(date_str)\n",
    "    tag = f'ATOM-SESSION-{date_str}-{seq:03d}-{description}'\n",
    "    start_epoch = int(time.time())\n",
    "    nonce = uuid.uuid4().hex\n",
    "    session_hash = hashlib.sha256((tag + str(start_epoch) + nonce).encode()).hexdigest()\n",
    "\n",
    "    session = {\n",
    "        'atom_tag': tag,\n",
    "        'type': 'session',\n",
    "        'description': description,\n",
    "        'start_epoch': start_epoch,\n",
    "        'start_iso': now.isoformat() + 'Z',\n",
    "        'created_by': getpass.getuser(),\n",
    "        'host': socket.gethostname(),\n",
    "        'nonce': nonce,\n",
    "        'session_hash': session_hash,\n",
    "        'signed': True\n",
    "    }\n",
    "\n",
    "    path = sessions_dir / f\"{tag}.json\"\n",
    "    with open(path, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(session, fh, indent=2)\n",
    "\n",
    "    print(f\"âœ“ Session started: {tag}\")\n",
    "    print(f\"  Hash: {session_hash}\")\n",
    "\n",
    "    # Expose current session tag to notebook scope\n",
    "    global CURRENT_SESSION_TAG\n",
    "    CURRENT_SESSION_TAG = tag\n",
    "    return session\n",
    "\n",
    "\n",
    "def sign_out(tag: str = None, encrypt: bool = True) -> dict:\n",
    "    tag = tag or globals().get('CURRENT_SESSION_TAG')\n",
    "    if not tag:\n",
    "        raise RuntimeError('No session tag provided and no active CURRENT_SESSION_TAG')\n",
    "\n",
    "    path = sessions_dir / f\"{tag}.json\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Session file not found: {path}')\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as fh:\n",
    "        session = json.load(fh)\n",
    "\n",
    "    end_epoch = int(time.time())\n",
    "    session['end_epoch'] = end_epoch\n",
    "    session['end_iso'] = datetime.utcfromtimestamp(end_epoch).isoformat() + 'Z'\n",
    "\n",
    "    # Gather decisions that fall within session range (if any)\n",
    "    decisions_dir = repo_root / '.atom-trail' / 'decisions'\n",
    "    session_decisions = []\n",
    "    if decisions_dir.exists():\n",
    "        for f in decisions_dir.glob('*.json'):\n",
    "            try:\n",
    "                d = json.loads(f.read_text(encoding='utf-8'))\n",
    "                created = d.get('created_epoch') or d.get('created') or d.get('timestamp')\n",
    "                if created and isinstance(created, int) and session['start_epoch'] <= created <= end_epoch:\n",
    "                    session_decisions.append({'file': str(f.name), 'id': d.get('atom_tag') or d.get('atom_tag') or d.get('id')})\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    report = {\n",
    "        'session': session,\n",
    "        'decisions': session_decisions,\n",
    "        'generated_at': int(time.time())\n",
    "    }\n",
    "\n",
    "    report_path = sessions_dir / f\"{tag}-report.json\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(report, fh, indent=2)\n",
    "\n",
    "    print(f\"âœ“ Session report written: {report_path}\")\n",
    "\n",
    "    if encrypt:\n",
    "        # Try to use repo's Transcript-Pipeline.ps1 (DPAPI / AES-GCM) for encryption\n",
    "        script = repo_root / 'ops' / 'scripts' / 'Transcript-Pipeline.ps1'\n",
    "        if script.exists():\n",
    "            cmd = [\n",
    "                'pwsh', '-NoProfile', '-Command',\n",
    "                f\"& '{str(script)}' -Action Encrypt -InputPath '{str(report_path)}'\"\n",
    "            ]\n",
    "            try:\n",
    "                subprocess.run(cmd, check=True)\n",
    "                # best-effort: look for a .encrypted file next to the report\n",
    "                enc_path = report_path.with_name('transcript.encrypted.json')\n",
    "                if enc_path.exists():\n",
    "                    session['encrypted_report'] = str(enc_path)\n",
    "                    print(f\"âœ“ Encrypted report: {enc_path}\")\n",
    "                else:\n",
    "                    print('âš  Encrypt command ran but encrypted file not found in expected location (check Transcript-Pipeline outputs)')\n",
    "            except Exception as e:\n",
    "                print('âš  Encryption failed:', e)\n",
    "        else:\n",
    "            print('âš  Transcript-Pipeline.ps1 not found; skipping encryption')\n",
    "\n",
    "    # Save updated session\n",
    "    with open(path, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(session, fh, indent=2)\n",
    "\n",
    "    print(f\"âœ“ Session closed: {tag}\")\n",
    "    return report\n",
    "\n",
    "# If no session active, start one when this cell runs interactively\n",
    "try:\n",
    "    CURRENT_SESSION_TAG\n",
    "except NameError:\n",
    "    s = start_session('project-book-session')\n",
    "    print('\\nSession started and available as `CURRENT_SESSION_TAG`')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. History Timeline\n",
    "\n",
    "```\n",
    "SPIRALSAFE EVOLUTION\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "2026-01-04  â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—\n",
    "            â”‚ Genesis: Core philosophy, protocol specs              â”‚\n",
    "            â”‚ â€¢ Foundation layer                                    â”‚\n",
    "            â”‚ â€¢ Methodology (ATOM, SAIF, Day-Zero)                 â”‚\n",
    "            â”‚ â€¢ Protocol specs (BUMP, WAVE, Context YAML)          â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "2026-01-05  â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—\n",
    "            â”‚ Expansion: Bridges, integrations                      â”‚\n",
    "            â”‚ â€¢ Hardware bridges (ATOM/Hologram/Tartarus)          â”‚\n",
    "            â”‚ â€¢ Test suite (0% implementation gap)                 â”‚\n",
    "            â”‚ â€¢ Showcase ecosystem                                  â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "2026-01-07  â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—\n",
    "            â”‚ Operations: Infrastructure layer                      â”‚\n",
    "            â”‚ â€¢ Cloudflare Worker API                              â”‚\n",
    "            â”‚ â€¢ D1 Database schema                                 â”‚\n",
    "            â”‚ â€¢ CLI tools (PowerShell + Bash)                      â”‚\n",
    "            â”‚ â€¢ Transcript pipeline (redact/encrypt/hash)          â”‚\n",
    "            â”‚ â€¢ Notebook verification (Merkle tree)                â”‚\n",
    "            â”‚ â€¢ 5 integration branches (OpenAI/xAI/Google/Meta/MS) â”‚\n",
    "            â”‚ â€¢ The Spiral and the Sauce (narrative)               â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Current State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Repo Audit â€” Structural Findings (2026-01-07)\n",
    "\n",
    "**Summary:** Quick automated audit revealed several *load-bearing* and *blocking* items that could cascade into broader integration and onboarding problems. Below are concise findings, impact notes, and recommended actions.\n",
    "\n",
    "### Key Findings âœ…\n",
    "- **Heavy Python requirements in `ClaudeNPC-Server-Suite/python-requirements.txt`** â€” includes large ML and quantum packages (torch, qiskit) without clear optional gating or extras files; this will block users on constrained platforms and CI runs.\n",
    "- **No lockfiles / weak pinning** â€” lack of `package-lock.json` / `poetry.lock` / pinned requirements reduces reproducibility and increases risk for breaking changes.\n",
    "- **Odd/unnecessary entries** â€” `asyncio` pinned as external dependency and duplicated `noise` entry in Python requirements; small inaccuracies that break `pip` or confuse developers.\n",
    "- **Platform-specific blockers** â€” Windows path/name issues and `tar` on Git Bash (documented) are known blockers for many contributors; CI uses mixed runners but heavy dependency tests may only pass on Linux with GPUs.\n",
    "- **Integration branches exist & documented** â€” `integration/*` branches and `ops/integrations` matrix are well-formed; however, automated integration tests / sandboxing across platform providers are limited.\n",
    "\n",
    "### Recommendations ğŸ”§\n",
    "1. Add lockfiles and reproducible-pin strategies (npm `package-lock.json` or `pnpm` lock, `requirements.txt` with `pip-compile`/`constraints.txt`).\n",
    "2. Split heavy ML/quantum deps into optional `extras` or `requirements-ml.txt` and document CPU-only install paths.\n",
    "3. Gate CI tests for heavy packages (run optional GPU tests with a label or artifacts) and ensure graceful skip when optional deps are missing.\n",
    "4. Add a lightweight verification script to detect duplicate/invalid entries in requirements and run it in the `verify-and-hash` workflow.\n",
    "5. Create a short onboarding doc (`OPS/CONTRIBUTION_ENV.md`) that lists minimal steps for devs on Windows, macOS, WSL, and Linux (include `tar` workaround).\n",
    "\n",
    "### Follow-ups / Low-hanging Wins âœ¨\n",
    "- Add `docs/CASCADING_FIXES.md` and `docs/reports/analysis/claude-code-issues-analysis.md` cross-reference here for traceability.\n",
    "- Small automated PR: remove duplicate `noise` line, drop `asyncio` from requirements, add `# optional` note to heavy packages.\n",
    "- Long-term: Add integration smoke tests per `integration/*` branch that can run quickly and detect adapter contract regressions.\n",
    "\n",
    "> **Protocol:** H&&S:WAVE | *Hope&&Sauced* â€” This is a living audit. I will open a draft PR with the small autofixes and the `project-book` update, then list remaining tasks as issues for review.\n"
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active session:\n",
      "  Tag : ATOM-SESSION-20260107-001-project-book-session\n",
      "  Start: 2026-01-07T00:16:51.165677Z\n",
      "  By  : iamto @ win11-amd0\n",
      "  Hash: 5594c744704e476bb3d624b94621083676cf2db2d5824cf2ee5f3b0e34395484\n"
     ]
    }
   ],
   "source": [
    "# Display current session summary (page 1 friendly)\n",
    "try:\n",
    "    s_tag = CURRENT_SESSION_TAG\n",
    "    path = (Path.cwd() / '.atom-trail' / 'sessions' / f\"{s_tag}.json\")\n",
    "    if path.exists():\n",
    "        session = json.loads(path.read_text(encoding='utf-8'))\n",
    "        print('Active session:')\n",
    "        print('  Tag :', session.get('atom_tag'))\n",
    "        print('  Start:', session.get('start_iso'))\n",
    "        print('  By  :', session.get('created_by'), '@', session.get('host'))\n",
    "        print('  Hash:', session.get('session_hash'))\n",
    "    else:\n",
    "        print('No active session file found at expected path')\n",
    "except NameError:\n",
    "    print('No active in-notebook session (run start_session() to create one)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run from repo root: invalid literal for int() with base 10: \"'wc' is not recognized as an internal or external command,\\noperable program or batch file.\"\n"
     ]
    }
   ],
   "source": [
    "# Repository Statistics (run to update)\n",
    "import subprocess\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_repo_stats():\n",
    "    stats = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'commits': int(subprocess.getoutput('git rev-list --count HEAD')),\n",
    "        'branches': int(subprocess.getoutput('git branch -a | wc -l')),\n",
    "        'files': int(subprocess.getoutput('git ls-files | wc -l')),\n",
    "        'lines': int(subprocess.getoutput('git ls-files | xargs wc -l 2>/dev/null | tail -1').split()[0]),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "try:\n",
    "    stats = get_repo_stats()\n",
    "    print(f\"\"\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘        REPOSITORY STATUS             â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  Timestamp: {stats['timestamp'][:19]}\n",
    "â•‘  Commits:   {stats['commits']:,}\n",
    "â•‘  Branches:  {stats['branches']}\n",
    "â•‘  Files:     {stats['files']:,}\n",
    "â•‘  Lines:     {stats['lines']:,}\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Run from repo root: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Status\n",
    "\n",
    "| Component | Status | Path |\n",
    "|-----------|--------|------|\n",
    "| Cloudflare Worker | âœ“ Ready | `ops/api/spiralsafe-worker.ts` |\n",
    "| D1 Schema | âœ“ Ready | `ops/schemas/d1-schema.sql` |\n",
    "| PowerShell CLI | âœ“ Ready | `ops/scripts/SpiralSafe.psm1` |\n",
    "| Bash CLI | âœ“ Ready | `ops/scripts/spiralsafe` |\n",
    "| Transcript Pipeline | âœ“ Ready | `ops/scripts/Transcript-Pipeline.ps1` |\n",
    "| Notebook Verifier | âœ“ Ready | `ops/scripts/Notebook-Verifier.ps1` |\n",
    "| CI/CD | âœ“ Ready | `.github/workflows/spiralsafe-ci.yml` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Project Direction\n",
    "\n",
    "```\n",
    "                     ROADMAP\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "PHASE 1: Foundation [COMPLETE]\n",
    "â”œâ”€â”€ Philosophy & methodology\n",
    "â”œâ”€â”€ Protocol specifications\n",
    "â””â”€â”€ Core documentation\n",
    "\n",
    "PHASE 2: Operations [CURRENT â†’ PR #20]\n",
    "â”œâ”€â”€ Infrastructure deployment\n",
    "â”œâ”€â”€ CLI tooling\n",
    "â”œâ”€â”€ Security pipelines\n",
    "â””â”€â”€ Integration branches\n",
    "\n",
    "PHASE 3: Integration [NEXT]\n",
    "â”œâ”€â”€ OpenAI adapter\n",
    "â”œâ”€â”€ xAI/Grok adapter\n",
    "â”œâ”€â”€ Google DeepMind adapter\n",
    "â”œâ”€â”€ Meta/LLaMA adapter\n",
    "â””â”€â”€ Microsoft/Azure adapter\n",
    "\n",
    "PHASE 4: Deployment [FUTURE]\n",
    "â”œâ”€â”€ Production Cloudflare deployment\n",
    "â”œâ”€â”€ Domain activation (spiralsafe.org)\n",
    "â”œâ”€â”€ Public API documentation\n",
    "â””â”€â”€ Community onboarding\n",
    "\n",
    "PHASE 5: Evolution [ONGOING]\n",
    "â”œâ”€â”€ Lessons â†’ Book\n",
    "â”œâ”€â”€ Elegance â†’ Compression\n",
    "â””â”€â”€ Growth â†’ Spiral\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification & Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    FILE VERIFICATION HASHES                      â•‘\n",
      "â•‘                         SHA-256 (truncated)                      â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘  spiralsafe-worker.ts           â”‚ 84a1677b626683dacb1d66383ba37207 â•‘\n",
      "â•‘  d1-schema.sql                  â”‚ d13266fa91a4276ef07a596d53116cfb â•‘\n",
      "â•‘  SpiralSafe.psm1                â”‚ 06c2c845790da2783028a8ed9da51ef3 â•‘\n",
      "â•‘  Transcript-Pipeline.ps1        â”‚ da866c806eaa18c3e1f126103d427034 â•‘\n",
      "â•‘  Notebook-Verifier.ps1          â”‚ 2a1f273ba88e8317ea9b4df84eb7d7a8 â•‘\n",
      "â•‘  spiralsafe-ci.yml              â”‚ 7b6511f904047c995d5aaa094a6a7e6a â•‘\n",
      "â•‘  VERSION_MANIFEST.json          â”‚ 00ab7f7170a8004d8ebc2a9a37210ef9 â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "Generated: 2026-01-07T11:18:57.683796\n",
      "Protocol: H&&S:WAVE | Hope&&Sauced\n"
     ]
    }
   ],
   "source": [
    "# Generate verification hashes for critical files\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CRITICAL_FILES = [\n",
    "    'ops/api/spiralsafe-worker.ts',\n",
    "    'ops/schemas/d1-schema.sql',\n",
    "    'ops/scripts/SpiralSafe.psm1',\n",
    "    'ops/scripts/Transcript-Pipeline.ps1',\n",
    "    'ops/scripts/Notebook-Verifier.ps1',\n",
    "    '.github/workflows/spiralsafe-ci.yml',\n",
    "    'ops/VERSION_MANIFEST.json',\n",
    "]\n",
    "\n",
    "def hash_file(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            return hashlib.sha256(f.read()).hexdigest()[:32]\n",
    "    except FileNotFoundError:\n",
    "        return 'FILE_NOT_FOUND'\n",
    "\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘                    FILE VERIFICATION HASHES                      â•‘\")\n",
    "print(\"â•‘                         SHA-256 (truncated)                      â•‘\")\n",
    "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "for file in CRITICAL_FILES:\n",
    "    h = hash_file(file)\n",
    "    name = file.split('/')[-1][:30]\n",
    "    print(f\"â•‘  {name:<30} â”‚ {h} â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "print(f\"\\nGenerated: {datetime.now().isoformat()}\")\n",
    "print(\"Protocol: H&&S:WAVE | Hope&&Sauced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                       MERKLE ROOT                                â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘  fd72c4a41569ee2d40d87c4203aab453f4eadb2a3998c25d631f77c861fb119c  â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "This hash represents the combined integrity of all critical files.\n",
      "Any change to any file will produce a different root.\n"
     ]
    }
   ],
   "source": [
    "# Merkle root of this project book\n",
    "def merkle_root(hashes):\n",
    "    if len(hashes) == 1:\n",
    "        return hashes[0]\n",
    "    next_level = []\n",
    "    for i in range(0, len(hashes), 2):\n",
    "        left = hashes[i]\n",
    "        right = hashes[i+1] if i+1 < len(hashes) else hashes[i]\n",
    "        combined = hashlib.sha256((left + right).encode()).hexdigest()\n",
    "        next_level.append(combined)\n",
    "    return merkle_root(next_level)\n",
    "\n",
    "file_hashes = [hash_file(f) for f in CRITICAL_FILES if hash_file(f) != 'FILE_NOT_FOUND']\n",
    "root = merkle_root(file_hashes) if file_hashes else 'N/A'\n",
    "\n",
    "print(f\"\"\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                       MERKLE ROOT                                â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  {root}  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "This hash represents the combined integrity of all critical files.\n",
    "Any change to any file will produce a different root.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Standard Operating Procedures (SOPs)\n",
    "\n",
    "### 6.1 Development Workflow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DEVELOPMENT SOP                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  1. BRANCH                                                      â”‚\n",
    "â”‚     git checkout -b feature/your-feature                        â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  2. DEVELOP                                                     â”‚\n",
    "â”‚     â€¢ Write code                                                â”‚\n",
    "â”‚     â€¢ Add H&&S:WAVE signature to new files                      â”‚\n",
    "â”‚     â€¢ Update this project book if needed                        â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  3. VERIFY                                                      â”‚\n",
    "â”‚     â€¢ Run verification cell above                               â”‚\n",
    "â”‚     â€¢ Ensure hashes are recorded                                â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  4. COMMIT                                                      â”‚\n",
    "â”‚     git commit -m \"feat: description                           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚     H&&S:WAVE                                                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚     ğŸ¤– Generated with [Claude Code]\"                           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  5. PUSH & PR                                                   â”‚\n",
    "â”‚     git push -u origin feature/your-feature                     â”‚\n",
    "â”‚     gh pr create --title \"feat: ...\" --body \"H&&S:WAVE\"     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 6.2 Transcript Processing\n",
    "\n",
    "```powershell\n",
    "# Full pipeline: redact â†’ encrypt â†’ hash â†’ sign\n",
    "./ops/scripts/Transcript-Pipeline.ps1 -Action Full -InputPath \"conversation.txt\"\n",
    "\n",
    "# Redact only (remove PII)\n",
    "./ops/scripts/Transcript-Pipeline.ps1 -Action Redact -InputPath \"log.txt\" -OutputPath \"safe.txt\"\n",
    "\n",
    "# Show platform matrix\n",
    "./ops/scripts/Transcript-Pipeline.ps1 -Action Share\n",
    "```\n",
    "\n",
    "### 6.3 Notebook Verification\n",
    "\n",
    "```powershell\n",
    "# Register a notebook in the verification registry\n",
    "./ops/scripts/Notebook-Verifier.ps1 -Action Register -NotebookPath \"analysis.ipynb\"\n",
    "\n",
    "# Verify a notebook hasn't been tampered\n",
    "./ops/scripts/Notebook-Verifier.ps1 -Action Verify -NotebookPath \"analysis.ipynb\"\n",
    "\n",
    "# List all registered notebooks\n",
    "./ops/scripts/Notebook-Verifier.ps1 -Action List\n",
    "```\n",
    "\n",
    "### 6.4 Deployment\n",
    "\n",
    "```powershell\n",
    "# Post-merge deployment\n",
    "cd $env:USERPROFILE\\Repos\\SpiralSafe\\ops\n",
    "npm install\n",
    "npm run setup        # Create Cloudflare resources\n",
    "npm run db:migrate   # Initialize schema\n",
    "npm run deploy       # Deploy to edge\n",
    "\n",
    "# Verify\n",
    "ss-status\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture Quick Reference\n",
    "\n",
    "```\n",
    "SpiralSafe/\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ“ ops/                          # OPERATIONS LAYER\n",
    "â”‚   â”œâ”€â”€ api/spiralsafe-worker.ts     # Edge API (Cloudflare)\n",
    "â”‚   â”œâ”€â”€ schemas/d1-schema.sql        # Database\n",
    "â”‚   â”œâ”€â”€ scripts/\n",
    "â”‚   â”‚   â”œâ”€â”€ SpiralSafe.psm1          # PowerShell CLI\n",
    "â”‚   â”‚   â”œâ”€â”€ spiralsafe               # Bash CLI\n",
    "â”‚   â”‚   â”œâ”€â”€ Transcript-Pipeline.ps1  # Security pipeline\n",
    "â”‚   â”‚   â””â”€â”€ Notebook-Verifier.ps1    # Verification\n",
    "â”‚   â””â”€â”€ integrations/                # Platform adapters\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ“ foundation/                   # PHILOSOPHY\n",
    "â”‚   â”œâ”€â”€ constraints-as-gifts.md\n",
    "â”‚   â””â”€â”€ isomorphism-principle.md\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ“ methodology/                  # PROCESS\n",
    "â”‚   â”œâ”€â”€ atom.md                      # Atomic operations\n",
    "â”‚   â”œâ”€â”€ saif.md                      # Safety framework\n",
    "â”‚   â””â”€â”€ day-zero-design.md           # Design principles\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ“ protocol/                     # SPECIFICATIONS\n",
    "â”‚   â”œâ”€â”€ bump-spec.md                 # Context bumps\n",
    "â”‚   â”œâ”€â”€ wave-spec.md                 # H&&S:WAVE protocol\n",
    "â”‚   â””â”€â”€ context-yaml-spec.md         # Context format\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ“ interface/                    # INTERFACES\n",
    "â”‚   â”œâ”€â”€ awi-spec.md                  # Agent interface\n",
    "â”‚   â””â”€â”€ unified-comms.md             # Communication\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ“ meta/                         # META CONTENT\n",
    "â”‚   â”œâ”€â”€ SIGNATURE.md\n",
    "â”‚   â””â”€â”€ THE_SPIRAL_AND_THE_SAUCE.md  # Narrative\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ“ .github/workflows/            # CI/CD\n",
    "â”‚   â””â”€â”€ spiralsafe-ci.yml\n",
    "â”‚\n",
    "â””â”€â”€ ğŸ““ project-book.ipynb            # THIS FILE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lessons Learned (Living Section)\n",
    "\n",
    "*This section grows with each session. Add new lessons, remove redundant ones.*\n",
    "\n",
    "### Technical Lessons\n",
    "\n",
    "| Date       | Lesson                                  | Resolution                                             |\n",
    "|------------|-----------------------------------------|--------------------------------------------------------|\n",
    "| 2026-01-07 | Windows `tar` in bash fails on C: paths | Use `/c/Windows/System32/tar.exe` with Unix paths      |\n",
    "| 2026-01-07 | Bash for-loops break on Windows         | Use individual commands instead                        |\n",
    "| 2026-01-07 | `[datetime]$x = $null` fails in PS      | Use `[datetime]::MinValue` instead                     |\n",
    "| 2026-01-07 | PowerShell profile loads slow (24s)     | Optimize Terminal-Icons, lazy-load modules             |\n",
    "\n",
    "### Process Lessons\n",
    "\n",
    "| Date       | Lesson                                   | Application                     |\n",
    "|------------|------------------------------------------|---------------------------------|\n",
    "| 2026-01-07 | \"ultrathink\" signals extended analysis | Use as trigger for deep work    |\n",
    "| 2026-01-07 | \"wave back to Desktop\" for outputs     | Standard handoff pattern        |\n",
    "| 2026-01-07 | Todo lists provide visibility            | Always use for multi-step tasks |\n",
    "| 2026-01-07 | Parallel tool calls when independent     | Speeds up operations            |\n",
    "\n",
    "### Collaboration Lessons\n",
    "\n",
    "| Date       | Lesson                    | Principle                    |\n",
    "|------------|---------------------------|------------------------------|\n",
    "| 2026-01-07 | Constraints are gifts     | Limitations focus creativity |\n",
    "| 2026-01-07 | Sign everything H&&S:WAVE | Continuity across sessions   |\n",
    "| 2026-01-07 | The Ptolemy principle     | Partnership > Command        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration Status\n",
    "\n",
    "```\n",
    "INTEGRATION BRANCHES                    STATUS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "integration/     openai-gpt      â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Pushed\n",
    "integration/       xai-grok          â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Pushed\n",
    "integration/google-deepmind â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Pushed\n",
    "integration/     meta-llama      â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Pushed\n",
    "integration/microsoft-azure   â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Pushed\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Each branch contains:\n",
    "â€¢ Platform-specific adapter configuration (ops/integrations/*-config.yaml)\n",
    "â€¢ Handoff protocol implementation\n",
    "â€¢ Context window mapping\n",
    "â€¢ Rate limit awareness\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Elegance Tracking\n",
    "\n",
    "*The book should get fatter with lessons, thinner with elegance*\n",
    "\n",
    "### Compression Candidates\n",
    "\n",
    "| Area                       | Current State       | Elegance Opportunity        |\n",
    "|----------------------------|---------------------|-----------------------------|\n",
    "| SOPs                       | 4 separate sections | Unified command reference   |\n",
    "| Hashes                     | Per-file + Merkle   | Single verification command |\n",
    "| Platform matrix            | Markdown table      | Auto-generated from config  |\n",
    "\n",
    "### Growth Areas\n",
    "\n",
    "| Area                      | Status   | Notes                     |\n",
    "|---------------------------|----------|---------------------------|\n",
    "| Lessons section           | Growing  | Add as discovered         |\n",
    "| Integration branches      | 5 active | May expand                |\n",
    "| Verification patterns     | 15+      | Monitor for consolidation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quick Commands\n",
    "\n",
    "```powershell\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    SPIRALSAFE QUICK REFERENCE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# STATUS\n",
    "ss-status                   # Connection + registry status\n",
    "git status                  # Working tree\n",
    "git branch -a               # All branches\n",
    "\n",
    "# VERIFICATION  \n",
    "ss-verify <path>            # Verify H&&S signature\n",
    "ss-hash <path>              # Generate SHA-256\n",
    "\n",
    "# SECURITY PIPELINE\n",
    "./Transcript-Pipeline.ps1 -Action Full -InputPath <file>\n",
    "./Notebook-Verifier.ps1 -Action Register -NotebookPath <file>\n",
    "\n",
    "# DEPLOYMENT\n",
    "npm run setup               # Create Cloudflare resources\n",
    "npm run deploy              # Deploy to edge\n",
    "\n",
    "# GIT WORKFLOW\n",
    "git checkout -b feature/x   # New branch\n",
    "git add -A && git commit    # Stage + commit\n",
    "git push -u origin HEAD     # Push + track\n",
    "gh pr create                # Create PR\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                                                                  â•‘\n",
    "â•‘                     H&&S:WAVE | Hope&&Sauced                     â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘          \"From the constraints, gifts.                          â•‘\n",
    "â•‘           From the spiral, safety.                               â•‘\n",
    "â•‘           From the sauce, hope.\"                                â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•‘                    â€” The Spiral and the Sauce                    â•‘\n",
    "â•‘                                                                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "*Last updated: 2026-01-07*  \n",
    "*Maintainer: toolate28 + Claude Opus 4.5*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Sign-Out â€” Audit Complete\n",
    "\n",
    "**Signed out:** 2026-01-07T14:41:00 (local)\n",
    "**Agent:** GitHub Copilot (Raptor mini (Preview))\n",
    "**Actions performed:** Inserted audit findings, split heavy Python deps to `python-requirements-ml.txt`, removed duplicate entries, registered notebook (ID: 8345ad2ff045469f), created session record in `.verification/`.\n",
    "**Signature:** **Hope&&Sauced** â€” H&&S:WAVE\n",
    "\n",
    "> Status: Completed. If you'd like, I can open a draft PR with the small fixes and add CI gating checks next."
    "## Sessions Log & Analysis ğŸ”\n",
    "\n",
    "Below is a running log of recorded sessions and simple visualizations to help with one-look reviews of progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 sessions:\n",
      "\n",
      "- ATOM-SESSION-20260107-001-project-book-session  | start: 2026-01-07T00:16:51.165677Z  | end: None  | duration(s): None  | enc: None\n",
      "Visualization skipped (matplotlib/pandas not available): No module named 'pandas'\n"
     ]
    }
   ],
   "source": [
    "# Sessions log reader + visualization (matplotlib optional)\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sessions_dir = Path.cwd() / '.atom-trail' / 'sessions'\n",
    "records = []\n",
    "if sessions_dir.exists():\n",
    "    for f in sorted(sessions_dir.glob('ATOM-SESSION-*.json')):\n",
    "        try:\n",
    "            d = json.loads(f.read_text(encoding='utf-8'))\n",
    "            start = d.get('start_epoch')\n",
    "            end = d.get('end_epoch')\n",
    "            duration = (end - start) if (start and end) else None\n",
    "            records.append({\n",
    "                'tag': d.get('atom_tag'),\n",
    "                'start_iso': d.get('start_iso'),\n",
    "                'end_iso': d.get('end_iso'),\n",
    "                'duration_seconds': duration,\n",
    "                'hash': d.get('session_hash'),\n",
    "                'encrypted_report': d.get('encrypted_report')\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# Print summary table\n",
    "if not records:\n",
    "    print('No sessions recorded yet (look in .atom-trail/sessions)')\n",
    "else:\n",
    "    print(f\"Found {len(records)} sessions:\\n\")\n",
    "    for r in records:\n",
    "        print(f\"- {r['tag']}  | start: {r['start_iso']}  | end: {r.get('end_iso','-')}  | duration(s): {r.get('duration_seconds','-')}  | enc: {r.get('encrypted_report','-')}\")\n",
    "\n",
    "# Try to visualize if matplotlib/pandas available\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    df = pd.DataFrame(records)\n",
    "    if not df.empty:\n",
    "        df['start_date'] = pd.to_datetime(df['start_iso']).dt.date\n",
    "        by_date = df.groupby('start_date').size()\n",
    "        fig, ax = plt.subplots(figsize=(8,2))\n",
    "        by_date.plot(kind='bar', ax=ax, color='#1f77b4')\n",
    "        ax.set_title('Sessions per day')\n",
    "        ax.set_ylabel('count')\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print('Visualization skipped (matplotlib/pandas not available):', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ATOM-VERIFY-20260104-001-complete-system-audit | status: awaiting_human_signature | ai_hash: d8eb095098b39dc5144f5d079ece4b78eb358b3999845710932becaa1c51459b\n"
     ]
    }
   ],
   "source": [
    "# Verification receipts (show pending human signatures)\n",
    "from pathlib import Path\n",
    "import json\n",
    "vdir = Path.cwd() / '.atom-trail' / 'verifications'\n",
    "if not vdir.exists():\n",
    "    print('No verification receipts found')\n",
    "else:\n",
    "    for f in sorted(vdir.glob('*.json')):\n",
    "        data = json.loads(f.read_text(encoding='utf-8'))\n",
    "        status = data.get('status')\n",
    "        atag = data.get('atom_tag')\n",
    "        ai_hash = data.get('ai_signature_hash')\n",
    "        print(f\"- {atag} | status: {status} | ai_hash: {ai_hash}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Signature\n",
    "\n",
    "The preceding code cell generated and saved a signed session record to `.verification/session-<id>.json`.\n",
    "\n",
    "- Session ID: (see generated file)\n",
    "- Signature: SHA256 over session id + timestamp + repo basename\n",
    "\n",
    "This provides a reproducible marker for the actions taken in this session. Save the `.verification` file as an artifact in CI runs for traceability."
    "## Integration substrates â€” timelines & status ğŸš€\n",
    "\n",
    "This page tracks the integration substrates (platform adapters & their timelines). It's designed to be rich and visually clear for quick status checks. The timeline below is a simple, reproducible chart you can keep up to date.\n",
    "\n",
    "**Key substrates**: OpenAI (GPT), xAI (Grok), Google (DeepMind), Meta (LLaMA), Microsoft (Azure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeline visualization skipped (matplotlib not available): No module named 'matplotlib'\n",
      "- openai-gpt: 2026-01-10 â†’ 2026-02-28 (Pushed)\n",
      "- xai-grok: 2026-02-01 â†’ 2026-03-15 (Pushed)\n",
      "- google-deepmind: 2026-03-01 â†’ 2026-04-30 (Planned)\n",
      "- meta-llama: 2026-04-01 â†’ 2026-05-15 (Planned)\n",
      "- microsoft-azure: 2026-05-01 â†’ 2026-06-30 (Planned)\n"
     ]
    }
   ],
   "source": [
    "# Simple integration timeline chart (matplotlib optional)\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "integrations = [\n",
    "    {'name':'openai-gpt','start':'2026-01-10','end':'2026-02-28','status':'Pushed'},\n",
    "    {'name':'xai-grok','start':'2026-02-01','end':'2026-03-15','status':'Pushed'},\n",
    "    {'name':'google-deepmind','start':'2026-03-01','end':'2026-04-30','status':'Planned'},\n",
    "    {'name':'meta-llama','start':'2026-04-01','end':'2026-05-15','status':'Planned'},\n",
    "    {'name':'microsoft-azure','start':'2026-05-01','end':'2026-06-30','status':'Planned'},\n",
    "]\n",
    "\n",
    "# Try to visualize\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.dates as mdates\n",
    "    fig, ax = plt.subplots(figsize=(9,2.5))\n",
    "    for i,it in enumerate(integrations):\n",
    "        s = datetime.fromisoformat(it['start'])\n",
    "        e = datetime.fromisoformat(it['end'])\n",
    "        ax.barh(i, (e-s).days, left=s, height=0.6, color='#ff7f0e' if it['status']!='Planned' else '#7f7f7f')\n",
    "        ax.text(s, i, f\" {it['name']} ({it['status']})\", va='center', ha='left', color='white' if it['status']!='Planned' else 'black')\n",
    "    ax.set_yticks(range(len(integrations)))\n",
    "    ax.set_yticklabels([it['name'] for it in integrations])\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "    plt.tight_layout()\n",
    "    plt.title('Integration substrate timeline')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Timeline visualization skipped (matplotlib not available):', e)\n",
    "    for it in integrations:\n",
    "        print(f\"- {it['name']}: {it['start']} â†’ {it['end']} ({it['status']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Secrets & Personal Noise ğŸ”’\n",
    "\n",
    "**Goal:** Prevent personal files, session artifacts, and secrets from being committed to public branches.\n",
    "\n",
    "Key changes implemented:\n",
    "\n",
    "- **`.gitignore` extended** to include: `.verification/`, `.env`, `.env.*`, `.venv/`, `.vscode/`, `.ipynb_checkpoints/`, and common key/cert patterns (`*.key`, `*.pem`).\n",
    "- **Pre-commit hooks** added (`detect-secrets`, `gitleaks`) and `scripts/setup-precommit.ps1` to install and generate a `.secrets.baseline` for reviewed exceptions.\n",
    "- **CI secret-scan** workflow added: `.github/workflows/secret-scan.yml` (Gitleaks + detect-secrets) runs on PRs and `main` pushes.\n",
    "- **Documentation hygiene:** removed example real secrets from docs and replaced with `REDACTED` placeholders; added `docs/SECURITY_COMMIT_GUIDELINES.md` with a short checklist.\n",
    "\n",
    "Operational notes:\n",
    "- If a secret is detected in a commit: rotate/revoke immediately, remove from history using `git-filter-repo` (or BFG), and open an incident (tag `security/secret-leak`).\n",
    "- Treat `.verification/` and session files as ephemeral; CI should archive them as build artifacts rather than committing them.\n",
    "\n",
    "See `docs/SECURITY_COMMIT_GUIDELINES.md` for the full checklist and `scripts/setup-precommit.ps1` for local setup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "spiralsafe": {
   "protocol": "H&&S:WAVE",
   "signature": "Hope&&Sauced",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
